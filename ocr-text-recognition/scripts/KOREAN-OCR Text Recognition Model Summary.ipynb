{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "sys.path.append(BASE_PATH)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from korean_ocr.layers import *\n",
    "from korean_ocr.data.generator import DataGenerator\n",
    "from korean_ocr.data.dataset import read_label_dataframe\n",
    "from korean_ocr.utils.serving import convert_model_to_inference_model\n",
    "from korean_ocr.utils.jamo import compose_unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 한글 데이터셋 구성하기 \\]\n",
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 한글 matplotlib 출력 세팅\n",
    "---\n",
    "\n",
    "Jupyter Notebook과 Matplotlib은 기본적으로 한글을 지원하지 않습니다. 한글이 출력되려면 아래와 같이 세팅을 해주어야 합니다.\n",
    "\n",
    "````bash\n",
    "# 나눔 폰트 설치하기\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 1. 나눔 폰트의 위치 가져오기 \n",
    "system_font = fm.findSystemFonts() # 현재 시스템에 설치된 폰트\n",
    "nanum_fonts = [\n",
    "    font for font in system_font if \"NanumBarunGothic.ttf\" in font]\n",
    "font_path = nanum_fonts[0] # 설정할 폰트의 경로\n",
    "\n",
    "# 2. 나눔 폰트로 설정하기\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc(\"font\",family=font_name)\n",
    "\n",
    "# 3. 폰트 재설정하기\n",
    "fm._rebuild()\n",
    "\n",
    "# 4. (optional) minus 기호 깨짐 방지\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset 구성하기\n",
    "----\n",
    "\n",
    "한국정보화진흥원(NIA)에서 제공하고 있는 데이터 셋은 크게 두가지가 있습니다.\n",
    "\n",
    "1. 손글씨 데이터 셋 : \n",
    "2. 인쇄 데이터 셋 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.data.dataset import read_label_dataframe, filter_out_dataframe\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_dir = \"../datasets/handwritten\"\n",
    "label_path = os.path.join(dataset_dir, \"dataset_info.json\")\n",
    "\n",
    "label_df = read_label_dataframe(label_path)\n",
    "label_df = filter_out_dataframe(label_df)\n",
    "\n",
    "# Train Dataset과 Validation Dataset을 구분\n",
    "train_df, valid_df = train_test_split(\n",
    "    label_df,test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 정보에 따라, 이미지와 라벨을 같이 읽어오는 클래스로 `OCRDataset`이 있습니다. 아래와 같이 이용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.data.dataset import OCRDataset\n",
    "\n",
    "# OCR DATASET 구성하기\n",
    "height = 64\n",
    "\n",
    "trainset = OCRDataset(train_df, height)\n",
    "validset = OCRDataset(valid_df, height)\n",
    "\n",
    "print(\"train data의 갯수 : \",len(trainset))\n",
    "print(\"valid data의 갯수 : \",len(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복수개의 이미지 가져오기\n",
    "images, texts = trainset[4:7]\n",
    "\n",
    "for image, text in zip(images, texts):\n",
    "    plt.title(text)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generator 만들기\n",
    "----\n",
    "\n",
    "위의 데이터 셋은 모델이 학습할 수 있는 형태로 가공 후, 배치 단위로 모델에게 주입시켜야 합니다. 이러한 작업은 아래의 `DataGenerator`에서 담당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.data.generator import DataGenerator\n",
    "from korean_ocr.utils.jamo import compose_unicode\n",
    "\n",
    "traingen = DataGenerator(trainset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator에서는 아래와 같이, Text를 Unicode 숫자로 변환 후 반환합니다. 그리고 배치 단위 학습을 위해, 이미지의 크기가 배치 단위로 모두 같도록 가장 가로로 긴 이미지를 기준으로, 가로 방향으로 검은색(pixel=0) 패딩을 채워줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_inputs, sample_outputs = traingen[4]\n",
    "sample_images = sample_inputs['images']\n",
    "sample_decoder_inputs = sample_inputs['decoder_inputs']\n",
    "\n",
    "for i in range(5):\n",
    "    image = sample_images[i]\n",
    "    plt.title('입력 이미지')\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])    \n",
    "    plt.show()    \n",
    "    print(\"outputs(text)    : \", compose_unicode(sample_outputs[i])[0])\n",
    "    print(\"outputs(unicode) : \", sample_outputs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 모델 구성하기 \\]\n",
    "---\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 전처리 파이프라인 추가\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.layers import PreprocessImage\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "K.clear_session()\n",
    "height = traingen.dataset.height\n",
    "\n",
    "inputs = Input(shape=(None, None), name='images')\n",
    "preprocess_layer = PreprocessImage(height=height, normalize=True,\n",
    "                                   name='encoder/preprocess')\n",
    "\n",
    "prep_out, prep_masks = preprocess_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test하기 위한 코드\n",
    "run = lambda x: K.get_session().run(x, {inputs:sample_images})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력값의 크기는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inputs Shape : \", inputs.get_shape())\n",
    "print(\"prep_out shape : \", prep_out.get_shape())\n",
    "print(\"prep_masks shape : \", prep_masks.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,2))\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    ax = fig.add_subplot(2,n,i)\n",
    "    ax.imshow(run(prep_out)[i,...,0],cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax2 = fig.add_subplot(2,n,i+n)\n",
    "    ax2.imshow((run(prep_masks)[i,...,0]),cmap='gray')\n",
    "    ax2.set_xticks([]); ax2.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolution Feature Extractor 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**출처 : Baoguang Shi, Robust Scene Text Recognition with Automatic Rectification**\n",
    "\n",
    "> In the SRN, the encoder has 7 convolutional layers, whose {filter size, number of fileters, stride, padding size} are respectively {3, 64, 1, 1}, {3, 128, 1, 1}, {3, 256, 1, 1}, {3, 256, 1, 1}, {3, 512, 1, 1}, {3, 512, 1, 1} and {2, 512, 1, 0}. The 1st, 2nd, 4th, 6th convolutional layers are each followed by a 2 x 2 max-pooling layer.\n",
    "\n",
    "우리는 위에서 제시한 모델에서 보다 빠른 수렴을 위해, Residual Connection과 Batch Normalization을 추가하였습니다. 그리고 한글은 조합형 언어이기 때문에, 종방향의 해상도가 좀 더 필요하다고 판단하여, height을 32가 아닌 64으로 설정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = ConvFeatureExtractor(name='encoder/feature_extractor')\n",
    "conv_maps, conv_masks = conv_layer([prep_out, prep_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력값의 크기는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inputs의 Shape : \", inputs.get_shape())\n",
    "print(\"-> conv maps의 Shape : \", conv_maps.get_shape())\n",
    "print(\"-> conv masks의 Shape : \", conv_masks.get_shape())\n",
    "\n",
    "print(\"\\n\\n실제 Sample Input의 Shape : \",sample_inputs['images'].shape)\n",
    "print(\"-> Conv Map의 Shape : \",\n",
    "      Model(inputs, conv_maps).predict(sample_inputs['images']).shape)\n",
    "print(\"-> Conv Masks의 Shape : \",run(conv_masks).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_conv_maps = run(conv_maps)\n",
    "fig = plt.figure(figsize=(8,2))\n",
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    ax = fig.add_subplot(2,n,i)\n",
    "    ax.imshow(ex_conv_maps[i,...,:3]/ex_conv_maps[i,...,:3].max(), cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax2 = fig.add_subplot(2,n,i+n)\n",
    "    ax2.imshow((run(conv_masks)[i,...,0]),cmap='gray')\n",
    "    ax2.set_xticks([]); ax2.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Map2Sequence 구성하기\n",
    "----\n",
    "\n",
    "**출처 : Baoguang Shi, Robust Scene Text Recognition with Automatic Rectification**\n",
    "\n",
    "> Specifically, the \"map-to-sequence\" operation takes out the columns of the maps in the left-to-right order, and flattens them into vectors. According to the translation invariance property of CNN, each vector corresponds to a local image region, i.e. receptiv field, and is a descriptor for that region.\n",
    "\n",
    "이미지 데이터는 4차원(batch, height, width, channel)으로 이루어져 있고, 텍스트 정보는 3차원(batch, width, height * channel)으로 이루어져 있습니다. 우리는 이미지 데이터를 통해 텍스트 정보를 추출해야 하므로, 4차원 정보를 3차원 정보로 변환해야 합니다. 이러한 작업을 수행하기 위해, Map2Sequence 연산을 구현하여 적용하였습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.layers import Map2Sequence\n",
    "\n",
    "m2s_layer = Map2Sequence(name='map_to_sequence')\n",
    "feat_maps, feat_masks = m2s_layer([conv_maps,conv_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력값의 크기는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"conv maps의 Shape : \", conv_maps.get_shape())\n",
    "print(\"-> feature maps의 Shape : \", feat_maps.get_shape())\n",
    "\n",
    "print(\"\\n\\n실제 Conv Map의 Shape : \",\n",
    "      Model(inputs, conv_maps).predict(sample_inputs).shape)\n",
    "print(\"-> feature maps의 Shape : \",\n",
    "      Model(inputs, feat_maps).predict(sample_inputs).shape)\n",
    "print(\"-> feat masks의 Shape : \",\n",
    "      run(feat_masks).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequence Encoder 구성하기\n",
    "----\n",
    "\n",
    "**출처 : Baoguang Shi, Robust Scene Text Recognition with Automatic Rectification**\n",
    "\n",
    "> Restricted by the sizes of the receptive fields, the feature sequence leverages limited image contexts. We further apply a two-layer Bidrectional Long-Short Term Memory(BLSTM) Network to the sequence, in order to model the long-term dependencies within the sequence. The BLSTM is a recurrent network that can analyze the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one. The output sequence is $h=(h_1, ...,h_L)$, where $L=W_{conv}$. \n",
    "\n",
    "CNN Network는 Receptive Field에 한정되어서만 정보를 읽어들이고, 이미지의 순서를 해석하지 못하기 때문에, 위의 문제들을 해결하기 위해 Sequence을 읽어들일 수 있는 BLSTM Layer을 추가하였습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.layers import SequenceEncoder\n",
    "\n",
    "num_depth = 2\n",
    "num_states = 128\n",
    "\n",
    "rnn_seqs = SequenceEncoder(\n",
    "    recurrent_cell='lstm', num_depth=num_depth,\n",
    "    num_states=num_states)([feat_maps,feat_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력값의 크기는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"feature maps의 Shape : \", feat_maps.get_shape())\n",
    "print(\"->rnn sequences의 Shape : \", rnn_seqs.get_shape())\n",
    "\n",
    "print(\"\\n\\n실제 feature maps의 Shape : \",\n",
    "      Model(inputs, feat_maps).predict(sample_inputs).shape)\n",
    "print(\"-> rnn sequences의 Shape : \",\n",
    "      Model(inputs, rnn_seqs).predict(sample_inputs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder Input Embedding 구성하기\n",
    "----\n",
    "\n",
    "초성 / 중성 / 종성 / 특수문자로 나누어 임베딩하는 방식으로 진행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.layers.text import CharEmbedding\n",
    "\n",
    "decoder_inputs = Input(shape=(None,), dtype=tf.int32, \n",
    "                       name='decoder_inputs')\n",
    "\n",
    "embedding_layer = CharEmbedding()\n",
    "dec_embeded, dec_masks = embedding_layer(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test하기 위한 코드\n",
    "run = lambda x: K.get_session().run(x, {inputs:sample_images,decoder_inputs:sample_decoder_inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력값의 크기는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"decoder inputs의 Shape : \", decoder_inputs.get_shape())\n",
    "print(\"dec_embeded의 Shape : \", dec_embeded.get_shape())\n",
    "print(\"dec_masks의 Shape : \", dec_masks.get_shape())\n",
    "\n",
    "print(\"\\n\\n실제 decoder inputs의 Shape : \", sample_decoder_inputs.shape)\n",
    "print(\"dec_embeded의 Shape : \", run(dec_embeded).shape)\n",
    "print(\"dec_masks의 Shape : \", run(dec_masks).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Decoder 구성하기\n",
    "----\n",
    "\n",
    "**출처 : Baoguang Shi, Robust Scene Text Recognition with Automatic Rectification**\n",
    "\n",
    "> The generation is a T-step process, at step $t$, the decoder computes a vector of attention weights $\\alpha_t \\in R^L$ via the attention process described in $\\alpha_t = Attend(s_{t-1}, \\alpha_{t-1}, h)$  where $s_{t-1}$ is the state variable of the GRU cell at the last step. For $t=1$, both $s_0$ and $\\alpha_0$ are zero vectors. Then, a glimpse $g_t$ is computed by linearly combining the vectors in h: $g_t = \\sum^L_{i=1} \\alpha_{ti}h_i$. Since $\\alpha_t$ has non-negative values that sum to one, it effectively controls where the decoder focused on. The state $s_{t-1}$ is updated via teh recurrent process of GRU: $s_t=GRU(l_{t-1}, g_t. s_{t-1})$. where $l_{t-1}$ is the (t-1) th ground-truth label in training, while in testing, it is the label predicted in the previous step, i.e. $\\hat l_{t-1}$. The probability distribution over the label space is estimated by: $\\hat y_t = softmax(W^t s_t).$ Following that, a character $\\hat l_t$ is predicted by taking the class with the highest probability. The label space includes all English alphanumeric characters, plus a special \"end-of-sequence\"(EOS) token, which ends the generation process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GRU Network : 글자 영상에서 순서에 맞게 텍스트를 해석\n",
    "2. Attention Network : GRU Network에서 필요한 정보만을 추출\n",
    "\n",
    "위 두 Network의 조합으로 Character Decoder를 구성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from korean_ocr.layers import AttentionDecoder\n",
    "attend_decoder = AttentionDecoder(num_states=num_states)\n",
    "\n",
    "states = attend_decoder([rnn_seqs, dec_embeded, feat_masks, dec_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력값의 크기는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rnn sequences의 Shape : \", rnn_seqs.get_shape())\n",
    "print(\"->attention state의 Shape : \", states.get_shape())\n",
    "\n",
    "sample_label = np.array([[1,3,5,2,1]])\n",
    "\n",
    "print(\"\\n\\n실제 rnn sequences의 Shape : \",\n",
    "      Model(inputs,\n",
    "            rnn_seqs).predict(sample_inputs).shape)\n",
    "print(\"-> attention state의 Shape : \",\n",
    "      Model([inputs, decoder_inputs],\n",
    "            states).predict(sample_inputs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chararcter Classification Layer 구성하기\n",
    "---\n",
    "\n",
    "한글 완성형 글자 수 (11,172자)와 EOS(End-Of-Sequence) 토큰을 더해 총 11,173자의 글자를 분류해야 합니다. 이경우 매우 Sparse하기 때문에, 모델의 학습에 문제가 발생할 수 있습니다. 이를 방지하기 위해서 우리는 한글이 조합형 글자라는 특징을 살려, 초성 / 중성 / 종성을 각각 나누어 Classification하고, 이를 합치는 방식으로 재구성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.layers import CharClassifier\n",
    "\n",
    "char_classifer = CharClassifier(num_fc=128)\n",
    "prediction = char_classifer(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력값의 크기는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"attention state의 Shape : \", states.get_shape())\n",
    "print(\"->output의 Shape : \", prediction.get_shape())\n",
    "\n",
    "sample_label = np.array([[1,3,5,2,1]])\n",
    "\n",
    "print(\"\\n\\n실제 attention state의 Shape : \",\n",
    "      Model([inputs, decoder_inputs], states).predict(sample_inputs).shape)\n",
    "print(\"-> output의 Shape : \",\n",
    "      Model([inputs, decoder_inputs], prediction).predict(sample_inputs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 모델 학습하기 \\]\n",
    "---\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모델 구성하기\n",
    "---\n",
    "\n",
    "위에서 설계한 모델은 아래의 코드로 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.model import build_ocr_model\n",
    "\n",
    "K.clear_session()\n",
    "ocr_model = build_ocr_model()\n",
    "ocr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 컴파일하기\n",
    "----\n",
    "\n",
    "모델에 학습에 필요한 옵티마이저, 목적 함수, 그리고 평가지표 등을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.model import compile_ocr_model\n",
    "\n",
    "ocr_model = compile_ocr_model(ocr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습시키기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korean_ocr.model import train_ocr_model\n",
    "\n",
    "train_ocr_model(ocr_model, data_dir=[\"../datasets/handwritten/\",\n",
    "                                     \"../datasets/printed/\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
