{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBoxes++ Tutorial\n",
    "___\n",
    "> 본 스크립트는 https://github.com/mvoelk/ssd_detectors 의 코드를 기반으로 제작되었습니다.<br>\n",
    "## - A Single-Shot Oriented Scene Text Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 스크립트는 \"TextBoxes++ : A Single-Shot Oriented Scene Text Detector\" 논문을 기반으로 하는 객체 인식의 end-to-end 튜토리얼 과정을 담고 있습니다. TextBoxes++에서는 기존 객체 인식에서 다양한 가로세로비를 가진 단어를 탐지하는데에 겪던 어려움을 효율적으로 해결하기 위해 디자인된 text-box layer를 사용합니다. TextBoxes++에서는 기존 객체인식에서 사용해왔던 수평적인 직사각형 형태의 바운딩 박스(bounding box)뿐만 아니라 일반적인 사각형 형태(Quadrilateral Rectangle)의 바운딩 박스와 회전된 직사각형 형태(Rotated Rectangle)의 바운딩 박스를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 튜토리얼을 순서대로 끝까지 진행하면 아래와 같은 결과를 얻을 수 있습니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/YgA4EJe.png\" width=\"800\">\n",
    "\n",
    "위 이미지는 SynthText 데이터셋을 사용하여 훈련시켰을 때의 결과를 보여줍니다. 파란색 박스는 바운딩 박스를 포함하는 최소한의 수평적인 직사각형을, 빨간색 박스는 회전된 직사각형 형태의 바운딩박스를, 초록색 박스는 일반적인 사각형 형태의 바운딩 박스를 의미합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert int(tf.__version__.split('.')[0]) ==1, \"Textboxes++은 tensorflow 2.0대 버전을 지원하지 않습니다.\"\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터셋은 `data`폴더 내에 각각 존재하고 있습니다. 우리가 사용할 Syntext 데이터셋을 불러오도록 하겠습니다. Syntext 데이터셋은 7,266,866개의 단어들로 이루어진 858,750장의 합성 이미지 파일이 200개의 폴더에 나누어져 담겨있으며 Ground-truth annotation은 `gt.mat`파일에 담겨있습니다. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋은 [Synth-text Downloads](https://www.robots.ox.ac.uk/~vgg/data/scenetext/) 에서 다운받으실 수 있습니다. 다운 받은 데이터는 `data/SynthText/` 디렉토리 내 저장해주시길 바랍니다.<br>\n",
    "\n",
    "압축을 해제하고 아래와 같은 디렉토리 구성으로 배치해 주시길 바랍니다.\n",
    "\n",
    "````\n",
    "data/SynthText/\n",
    "        |- 1/\n",
    "            |- ant+hill_100_0.jpg\n",
    "            |- ant+hill_100_1.jpg\n",
    "            |- ...\n",
    "        |- 2/\n",
    "            |- ...\n",
    "        |- 3/\n",
    "            |- ...\n",
    "        |- ....\n",
    "        |- gt.mat\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.synthtext import GTUtility\n",
    "gt_util = GTUtility('data/SynthText/', polygon=True)\n",
    "\n",
    "print(gt_util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 통해, 현재 데이터를 시각화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx, img, gt = gt_util.sample()\n",
    "\n",
    "plt.imshow(img)\n",
    "gt_util.plot_gt(gt, show_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Validation은 아래와 같이 `gt_util.split` 메소드를 이용하여 쉽게 나눌 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_util_train, gt_util_val = gt_util.split(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBoxes++의 모델은 기본적으로 VGG-16의 13개의 layer와 그 뒤로 10개의 convolutional layer가 추가로 붙어 있으며, 중간 중간 layer와 연결된 6개의 Text-box layers로 구성되어 있습니다. Text-box layer의 각 layer는 글자의 존재유무, 수평적인 직사각형 바운딩 박스, 회전된 직사각형 형태의 바운딩 박스, 일반적인 사각형 형태의 바운딩 박스에 대한 정보를 predict합니다. Text-box layer의 뒤에는 non-maximum suppression이 수행됩니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/V1rfeNo.png\" width=\"800\" heigth=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build text-box layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 먼저 Text-box layer를 구성해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tbpp384_body(x):\n",
    "    \n",
    "    source_layers = []\n",
    "    \n",
    "    # Block 1\n",
    "    x = Conv2D(64, 3, strides=1, padding='same', name='conv1_1', activation='relu')(x)\n",
    "    x = Conv2D(64, 3, strides=1, padding='same', name='conv1_2', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2, strides=2, padding='same', name='pool1')(x)\n",
    "    # Block 2\n",
    "    x = Conv2D(128, 3, strides=1, padding='same', name='conv2_1', activation='relu')(x)\n",
    "    x = Conv2D(128, 3, strides=1, padding='same', name='conv2_2', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2, strides=2, padding='same', name='pool2')(x)\n",
    "    # Block 3\n",
    "    x = Conv2D(256, 3, strides=1, padding='same', name='conv3_1', activation='relu')(x)\n",
    "    x = Conv2D(256, 3, strides=1, padding='same', name='conv3_2', activation='relu')(x)\n",
    "    x = Conv2D(256, 3, strides=1, padding='same', name='conv3_3', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2, strides=2, padding='same', name='pool3')(x)\n",
    "    # Block 4\n",
    "    x = Conv2D(512, 3, strides=1, padding='same', name='conv4_1', activation='relu')(x)\n",
    "    x = Conv2D(512, 3, strides=1, padding='same', name='conv4_2', activation='relu')(x)\n",
    "    x = Conv2D(512, 3, strides=1, padding='same', name='conv4_3', activation='relu')(x)\n",
    "    source_layers.append(x)\n",
    "    x = MaxPooling2D(pool_size=2, strides=2, padding='same', name='pool4')(x)\n",
    "    # Block 5\n",
    "    x = Conv2D(512, 3, strides=1, padding='same', name='conv5_1', activation='relu')(x)\n",
    "    x = Conv2D(512, 3, strides=1, padding='same', name='conv5_2', activation='relu')(x)\n",
    "    x = Conv2D(512, 3, strides=1, padding='same', name='conv5_3', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=3, strides=1, padding='same', name='pool5')(x)\n",
    "    # FC6\n",
    "    x = Conv2D(1024, 3, strides=1, dilation_rate=(6, 6), padding='same', name='fc6', activation='relu')(x)\n",
    "    # FC7\n",
    "    x = Conv2D(1024, 1, strides=1, padding='same', name='fc7', activation='relu')(x)\n",
    "    source_layers.append(x)\n",
    "    # Block 6\n",
    "    x = Conv2D(256, 1, strides=1, padding='same', name='conv6_1', activation='relu')(x)\n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Conv2D(512, 3, strides=2, padding='valid', name='conv6_2', activation='relu')(x)\n",
    "    source_layers.append(x)\n",
    "    # Block 7\n",
    "    x = Conv2D(128, 1, strides=1, padding='same', name='conv7_1', activation='relu')(x)\n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Conv2D(256, 3, strides=2, padding='valid', name='conv7_2', activation='relu')(x)\n",
    "    source_layers.append(x)\n",
    "    # Block 8\n",
    "    x = Conv2D(128, 1, strides=1, padding='same', name='conv8_1', activation='relu')(x)\n",
    "    x = Conv2D(256, 3, strides=1, padding='valid', name='conv8_2', activation='relu')(x)\n",
    "    source_layers.append(x)\n",
    "    # Block 9\n",
    "    x = Conv2D(128, 1, strides=1, padding='same', name='conv9_1', activation='relu')(x)\n",
    "    x = Conv2D(256, 3, strides=1, padding='valid', name='conv9_2', activation='relu')(x)\n",
    "    source_layers.append(x)\n",
    "    \n",
    "    return source_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid multibox head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text-box layer의 각 layer는 글자의 존재유무(`mbox_conf`), 글자를 포괄하는 최소 직사각형 바운딩 박스(`mbox_loc`), 일반적인 사각형 형태의 바운딩 박스에 대한 정보(`mbox_quad`), 회전된 직사각형 형태의 바운딩 박스(`mbox_rbox`)를 predict합니다. 이를 메소드로 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multibox_head(source_layers, num_priors, normalizations=None, softmax=True):\n",
    "    \n",
    "    num_classes = 2\n",
    "    class_activation = 'softmax' if softmax else 'sigmoid'\n",
    "\n",
    "    mbox_conf = []\n",
    "    mbox_loc = []\n",
    "    mbox_quad = []\n",
    "    mbox_rbox = []\n",
    "    for i in range(len(source_layers)):\n",
    "        x = source_layers[i]\n",
    "        name = x.name.split('/')[0]\n",
    "        \n",
    "        # normalize\n",
    "        if normalizations is not None and normalizations[i] > 0:\n",
    "            name = name + '_norm'\n",
    "            x = Normalize(normalizations[i], name=name)(x)\n",
    "            \n",
    "        # confidence\n",
    "        name1 = name + '_mbox_conf'\n",
    "        x1 = Conv2D(num_priors[i] * num_classes, (3, 5), padding='same', name=name1)(x)\n",
    "        x1 = Flatten(name=name1+'_flat')(x1)\n",
    "        mbox_conf.append(x1)\n",
    "\n",
    "        # location, Delta(x,y,w,h)\n",
    "        name2 = name + '_mbox_loc'\n",
    "        x2 = Conv2D(num_priors[i] * 4, (3, 5), padding='same', name=name2)(x)\n",
    "        x2 = Flatten(name=name2+'_flat')(x2)\n",
    "        mbox_loc.append(x2)\n",
    "        \n",
    "        # quadrilateral, Delta(x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "        name3 = name + '_mbox_quad'\n",
    "        x3 = Conv2D(num_priors[i] * 8, (3, 5), padding='same', name=name3)(x)\n",
    "        x3 = Flatten(name=name3+'_flat')(x3)\n",
    "        mbox_quad.append(x3)\n",
    "\n",
    "        # rotated rectangle, Delta(x1,y1,x2,y2,h)\n",
    "        name4 = name + '_mbox_rbox'\n",
    "        x4 = Conv2D(num_priors[i] * 5, (3, 5), padding='same', name=name4)(x)\n",
    "        x4 = Flatten(name=name4+'_flat')(x4)\n",
    "        mbox_rbox.append(x4)\n",
    "        \n",
    "    mbox_conf = concatenate(mbox_conf, axis=1, name='mbox_conf')\n",
    "    mbox_conf = Reshape((-1, num_classes), name='mbox_conf_logits')(mbox_conf)\n",
    "    mbox_conf = Activation(class_activation, name='mbox_conf_final')(mbox_conf)\n",
    "    \n",
    "    mbox_loc = concatenate(mbox_loc, axis=1, name='mbox_loc')\n",
    "    mbox_loc = Reshape((-1, 4), name='mbox_loc_final')(mbox_loc)\n",
    "    \n",
    "    mbox_quad = concatenate(mbox_quad, axis=1, name='mbox_quad')\n",
    "    mbox_quad = Reshape((-1, 8), name='mbox_quad_final')(mbox_quad)\n",
    "    \n",
    "    mbox_rbox = concatenate(mbox_rbox, axis=1, name='mbox_rbox')\n",
    "    mbox_rbox = Reshape((-1, 5), name='mbox_rbox_final')(mbox_rbox)\n",
    "\n",
    "    predictions = concatenate([mbox_loc, mbox_quad, mbox_rbox, mbox_conf],\n",
    "                              axis=2, name='predictions')\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TextBoxes++ 384 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위에서 구현한 메소드들을 토대로 TBPP384 model을 구성해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TBPP384(input_shape=(384, 384, 3), softmax=True):\n",
    "    \"\"\"\n",
    "    TextBoxes++384 architecture.\n",
    "\n",
    "    # Arguments\n",
    "        input_shape: Shape of the input image.\n",
    "    \n",
    "    # References\n",
    "        - [TextBoxes++: A Single-Shot Oriented Scene Text Detector](https://arxiv.org/abs/1801.02765)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TBPP body\n",
    "    x = input_tensor = Input(shape=input_shape)\n",
    "    source_layers = tbpp384_body(x)\n",
    "    \n",
    "    num_maps = len(source_layers)\n",
    "    \n",
    "    # Add multibox head for classification and regression\n",
    "    num_priors = [14] * num_maps\n",
    "    normalizations = [1] * num_maps\n",
    "    output_tensor = multibox_head(source_layers, num_priors, normalizations, softmax)\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    \n",
    "    # parameters for prior boxes\n",
    "    model.image_size = input_shape[:2]\n",
    "    model.source_layers = source_layers\n",
    "    \n",
    "    # Hyper-Parameter에 대한 설명 + Hyper-Parameter를 변경하는 팁\n",
    "    model.aspect_ratios = [[1,2,3,5,1/2,1/3,1/5] * 2] * num_maps\n",
    "    model.shifts = [[(0.0, -0.25)] * 7 + [(0.0, 0.25)] * 7] * num_maps\n",
    "    model.special_ssd_boxes = False\n",
    "    model.scale = 0.5\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = TBPP384(softmax=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모델은 미리 구현해 놓은 모듈을 사용해 아래와 같이 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "from utils.model import TBPP384\n",
    "model = TBPP384(softmax=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting prior boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "객체 인식을 위해 Text-box layer의 각 layer마다 가상의 box인 prior box를 설정해줍니다. 이 prior box는 multibox_head가 물체를 잡을 때 기준이 되는 box로, prior box에 물체가 일정 수치 이상 겹쳤을 때에만 동작하도록 구성됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text-box layer의 각 feature map 별 prior box의 중심좌표를 시각화해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prior import PriorMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_layers_names = [l.name.split('/')[0] for l in model.source_layers]\n",
    "for i in range(len(source_layers_names)):\n",
    "    num_maps = len(source_layers_names)\n",
    "    min_dim = np.min(model.image_size)\n",
    "    min_ratio = 10 # 15\n",
    "    max_ratio = 100 # 90\n",
    "    s = np.linspace(min_ratio, max_ratio, num_maps+1) * min_dim / 100.\n",
    "    minmax_sizes = [(round(s[i]), round(s[i+1])) for i in range(len(s)-1)]\n",
    "\n",
    "    \n",
    "    layer = model.source_layers[i]\n",
    "    map_h, map_w = map_size = layer.get_shape().as_list()[1:3]\n",
    "    m = PriorMap(source_layer_name=source_layers_names[i],\n",
    "                 image_size=model.image_size,\n",
    "                 map_size=map_size,\n",
    "                 minmax_size=minmax_sizes[i],\n",
    "                 variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                 aspect_ratios=model.aspect_ratios[i],\n",
    "                 shift=model.shifts[i],\n",
    "                 step=None,\n",
    "                 special_ssd_box=False,\n",
    "                 clip=False)\n",
    "    \n",
    "    m.compute_priors()\n",
    "    m.plot_locations()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 중심좌표당 14개의 prior box의 모습은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_boxes([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting prior utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior box와 물체가 겹쳤는지를 판단하는 지표로 일반적으로 IOU(intersection over union)를 사용합니다. <br>\n",
    "Textboxes++의 경우 수평적인 직사각형 형태의 바운딩 박스를 다루는 것이 아니므로, 이를 계산하기 매우 까다롭습니다. <br>\n",
    "계산문제를 해결하기 위해, 우리는 일반적인 사각형 혹은 회전된 직사각형 형태의 바운딩 박스를 모두 포함하는 최소한의 수평적인 직사각형 형태의 바운딩 박스를 기준으로 iou를 계산합니다. <br> \n",
    "IOU계산을 위해 우리는 기존 형태의 바운딩 박스에서 수평적인 직사각형 행태의 바운딩 박스로 변환하는 encoding과정과 다시 기존 형태의 바운딩 박스로 변환해주는 decoding 과정이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IOU계산, non-maximum suppression을 진행하는 과정은 미리 구현해 놓은 모듈을 사용해 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prior import PriorUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_util = PriorUtil(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training 단계로 넘어가기 전에, 샘플 이미지에 prior box들을 시각화 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = cv2.resize(img, (384,384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for m in prior_util.prior_maps:\n",
    "    plt.figure(figsize=[8]*2)\n",
    "    plt.imshow(sample_img)\n",
    "    m.plot_locations()\n",
    "    m.plot_boxes([0, 10, 100])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import TBPP384\n",
    "from utils.prior import PriorUtil\n",
    "from utils.data import InputGenerator\n",
    "from utils.training import TBPPFocalLoss\n",
    "from utils.training import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Generater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data augmentation을 진행하는 Input Generator입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "gen_train = InputGenerator(gt_util_train, prior_util, batch_size, model.image_size)\n",
    "gen_val = InputGenerator(gt_util_val, prior_util, batch_size*4, model.image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`next`를 통해 다음 이미지를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = gen_train.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imags, labels = next(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 얻을 수 있는 label에 대한 정보는 다음과 같습니다.\n",
    "\n",
    "`(batch_size, prior box의 갯수, predict한 결과의 갯수)`\n",
    "* predict한 결과의 갯수 : \n",
    "        confidence : background / existence\n",
    "        location : Delta(x,y,w,h)\n",
    "        quadrilateral : Delta(x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "        rotated rectangle : Delta(x1,y1,x2,y2,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Loss**\n",
    "\n",
    "우리가 학습해야 하는 것은 위치를 추론하는 Regressor와 사물인지 아닌지를 구분하는 Classifer입니다. TextBoxes++ 논문에서, Regressor의 경우에는 SmoothL1이라 불리는 Loss를 사용하고, Classifier은 분류모델에서 주로 사용하는 Cross-Entropy Loss를 사용합니다. <br><br>\n",
    "$$\n",
    "smooth_{L1}(x) = \\begin{cases}\n",
    "0.5x^2, \\mbox{  if  } |x| <1\\\\\n",
    "|x| - 0.5 \\mbox{   otherwise,}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "본 스크립트에서는 Cross=Entropy Loss대신 Focal Loss를 사용하도록 하겠습니다. Focal Loss는 기존의 Hard Negative Sampling 방법 대신 이용할 수 있는 방법론으로, Loss 식을 바꾸는 것 만으로도 성능을 개선할 수 있습니다. <br><br>\n",
    "Focal Loss 는 잘 분류된 예제들에 대해서는 작은 가중치를 부여하는 반면 분류하기 어려운 일부 예제들에는 큰 가중치를 부여해서 학습을 어려운 예제에 집중시킴으로써 객체 인식의 고질적인 문제점인 극단적인 class imbalance 문제를 해결합니다.\n",
    "\n",
    "$$\n",
    "p_t = \\begin{cases}\n",
    "p && \\mbox{if y = 1} \\\\\n",
    "1-p && \\mbox{otherwise,}\n",
    "\\end{cases} \\\\\n",
    "FL(p_t) = - (1-p_t)^\\gamma log(p_t).\n",
    "$$\n",
    "\n",
    "<img src=\"https://i.imgur.com/lmMkV3X.png\" width=\"400\" heigth=\"300\">\n",
    "\n",
    "$\\gamma$ > 0 으로 셋팅하면 상대적으로 잘 분류된 예제들(well-classified examples: $p_t$ > 0.5)의 로스를 줄일 수 있고 이를 통해 잘못 분류된 예제들에게 더욱 집중할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import TBPPFocalLoss\n",
    "loss = TBPPFocalLoss(lambda_conf=10000.0, lambda_offsets=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Metrics**\n",
    "\n",
    "Metric으로는 흔히 사용하는 F1 score를 사용하도록 하겠습니다.\n",
    "\n",
    "$$\n",
    "recall = \\frac{TP}{TP+FN},~precision = \\frac{TP}{TP+FP} \\\\\n",
    "F = 2* \\frac{recall * precision}{recall + precision}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 class는 미리 구현해놓은 모듈을 통해 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TextBoxes++ 논문에서는 Optimizer로 Adam optimizer를 사용했지만, 본 스크립트에서는 보다 더 빠른 training을 위해 SGD를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.SGD(lr=1e-3, momentum=0.9, decay=0, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 epoch마다 checkpoints들을 저장하기 위해 checkpoint를 저장할 경로를 지정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "experiment = 'tbpp384fl_synthtext'\n",
    "checkdir = './checkpoints/' + time.strftime('%Y%m%d%H%M') + '_' + experiment\n",
    "if not os.path.exists(checkdir):\n",
    "    os.makedirs(checkdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* end-to-end training 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "gen_train = InputGenerator(gt_util_train, prior_util, batch_size, model.image_size)\n",
    "gen_val = InputGenerator(gt_util_val, prior_util, batch_size*4, model.image_size)\n",
    "\n",
    "checkdir = './checkpoints/' + time.strftime('%Y%m%d%H%M') + '_' + experiment\n",
    "if not os.path.exists(checkdir):\n",
    "    os.makedirs(checkdir)\n",
    "\n",
    "with open(checkdir+'/source.py','wb') as f:\n",
    "    source = ''.join(['# In[%i]\\n%s\\n\\n' % (i, In[i]) for i in range(len(In))])\n",
    "    f.write(source.encode())\n",
    "\n",
    "optim = tf.keras.optimizers.SGD(lr=1e-3, momentum=0.9, decay=0, nesterov=True)\n",
    "# optim = tf.keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=0.001, decay=0.0)\n",
    "\n",
    "# weight decay\n",
    "regularizer = tf.keras.regularizers.l2(5e-4) # None if disabled\n",
    "#regularizer = None\n",
    "for l in model.layers:\n",
    "    if l.__class__.__name__.startswith('Conv'):\n",
    "        l.kernel_regularizer = regularizer\n",
    "\n",
    "loss = TBPPFocalLoss(lambda_conf=10000.0, lambda_offsets=1.0)\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss.compute, metrics=loss.metrics)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkdir+'/weights.{epoch:03d}.h5', \n",
    "                    verbose=1, save_weights_only=True),\n",
    "    Logger(checkdir)\n",
    "]\n",
    "\n",
    "print(checkdir.split('/')[-1])\n",
    "history = model.fit_generator(\n",
    "        gen_train.generate(),\n",
    "        epochs=epochs, \n",
    "        steps_per_epoch=int(gen_train.num_batches/4), \n",
    "        callbacks=callbacks,\n",
    "        validation_data=gen_val.generate(), \n",
    "        validation_steps=int(gen_val.num_batches/4),\n",
    "        workers=cpu_count(), \n",
    "        use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from utils.model import TBPP384\n",
    "from utils.metric import fscore\n",
    "from utils.metric import evaluate_polygonal_results\n",
    "from utils.bboxes import rbox3_to_polygon, polygon_to_rbox3\n",
    "from utils.vis import plot_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 평가를 위해 위에서 훈련한 모델을 불러오겠습니다. `weights_path`에는 훈련과정에서 checkpoint를 이용해 저장한 모델의 경로를 설정해 주세요.\n",
    "\n",
    "```python\n",
    "# 예시\n",
    "weights_path = './checkpoints/201911120606_tbpp384fl_synthtext/weights.010.h5'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBoxes++\n",
    "K.clear_session()\n",
    "model = TBPP384(softmax=False)\n",
    "weights_path = ''\n",
    "plot_name = 'tbpp384fl_synthtext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weights_path)\n",
    "checkdir = os.path.dirname(weights_path)\n",
    "prior_util = PriorUtil(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련 결과를 시각화 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, inputs, images, data = gt_util_val.sample_random_batch(1024)\n",
    "\n",
    "preds = model.predict(inputs, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    res = prior_util.decode(preds[i], confidence_threshold, fast_nms=False)\n",
    "    bbox = res[:,0:4]\n",
    "    quad = res[:,4:12]\n",
    "    rbox = res[:,12:17]\n",
    "    #print(bbox)\n",
    "    \n",
    "    plt.figure(figsize=[8]*2)\n",
    "    plt.imshow(images[i])\n",
    "    ax = plt.gca()\n",
    "    for j in range(len(bbox)):\n",
    "        #ax.add_patch(plt.Polygon(p, fill=False, edgecolor='r', linewidth=1))\n",
    "        plot_box(bbox[j]*384, box_format='xyxy', color='b')\n",
    "        plot_box(np.reshape(quad[j],(-1,2))*384, box_format='polygon', color='r')\n",
    "        plot_box(rbox3_to_polygon(rbox[j])*384, box_format='polygon', color='g')\n",
    "        plt.plot(rbox[j,[0,2]]*384, rbox[j,[1,3]]*384, 'oc', markersize=4)\n",
    "    #prior_util.plot_gt()\n",
    "    #prior_util.plot_results(res)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find proper threshold ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 적절한 threshold ratio를 찾기 위해 grid search 방식을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.05, 1, 0.05)\n",
    "\n",
    "fmes_grid = np.zeros((len(steps)))\n",
    "\n",
    "for i, t in enumerate(steps):\n",
    "    results = [prior_util.decode(p, t) for p in preds]\n",
    "    TP, FP, FN = evaluate_polygonal_results([g[:,0:8] for g in data], [d[:,4:12] for d in results])\n",
    "    recall = TP / (TP+FN)\n",
    "    precision = TP / (TP+FP)\n",
    "    fmes = fscore(precision, recall)\n",
    "    fmes_grid[i] = fmes\n",
    "    print('threshold %.2f f-measure %.2f' % (t, fmes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold ratio에 따른 f-score는 아래 그래프와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = np.argmax(fmes_grid)\n",
    "print(steps[max_idx], fmes_grid[max_idx])\n",
    "plt.figure(figsize=[12,6])\n",
    "plt.plot(steps, fmes_grid)\n",
    "plt.plot(steps[max_idx], fmes_grid[max_idx], 'or')\n",
    "plt.xticks(steps)\n",
    "plt.grid()\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('f-measure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* precision, recall, f-measure를 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confidence_threshold는 hyperparameter입니다. 윗 단계의 그래프를 보고 적절한 confidence_threshold를 설정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "max_samples = gt_util_val.num_samples\n",
    "max_samples = batch_size * 32\n",
    "\n",
    "test_gt = []\n",
    "test_results = [] \n",
    "\n",
    "for i in tqdm(range(int(np.ceil(max_samples/batch_size)))):\n",
    "    inputs, data = gt_util_val.sample_batch(batch_size, i)\n",
    "    preds = model.predict(inputs, batch_size, verbose=0)\n",
    "    res = [prior_util.decode(p, confidence_threshold) for p in preds]\n",
    "    test_gt.extend(data)\n",
    "    test_results.extend(res)\n",
    "\n",
    "TP, FP, FN = evaluate_polygonal_results([g[:,0:8] for g in test_gt], [d[:,4:12] for d in test_results])\n",
    "recall = TP / (TP+FN)\n",
    "precision = TP / (TP+FP)\n",
    "fmes = fscore(precision, recall)\n",
    "\n",
    "print('samples train     %i' % (gt_util_train.num_samples))\n",
    "print('samples val       %i' % (gt_util_val.num_samples))\n",
    "\n",
    "print('samples           %i' % (max_samples))\n",
    "print('threshold         %0.3f' % (confidence_threshold))\n",
    "print('precision         %0.3f' % (precision))\n",
    "print('recall            %0.3f' % (recall))\n",
    "print('f-measure         %0.3f' % (fmes))\n",
    "\n",
    "trainable_count = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "non_trainable_count = int(np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "\n",
    "print('trainable parameters     %10i' %(trainable_count))\n",
    "print('non-trainable parameters %10i' %(non_trainable_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
